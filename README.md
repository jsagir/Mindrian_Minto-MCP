# ğŸ›ï¸ Minto Pyramid Logic MCP Server v3.0

> **Reasoning Orchestrator:** Research-grade sequential thinking engine implementing Pyramid Principle as a first-class execution framework

## ğŸ¯ What's New in v3.0

### **From Tools to Orchestrator**

**v2.0** provided individual tools (search, analyze, validate)  
**v3.0** orchestrates them into a **complete reasoning pipeline** with:

- âœ… **Explicit Planning Stage** - Generate ReasoningPlan before execution
- âœ… **MECE Validation** - Automated scoring and quality gates
- âœ… **Evidence Orchestration** - Parallel search with provenance tracking
- âœ… **Contradiction Detection** - NLI-based consistency checks
- âœ… **Critique Loops** - Systematic evaluation and revision
- âœ… **State Management** - Full run history and artifacts
- âœ… **Metrics & Observability** - Quality scores and tracing

---

## ğŸ—ï¸ Architecture: 5-Stage Pipeline

```
1. PLAN          â†’  2. EXECUTE      â†’  3. SYNTHESIZE  â†’  4. CRITIQUE     â†’  5. FINALIZE
plan_pyramid        run_plan_stage      synthesize_       critique_          finalize_
                                        pyramid           pyramid_tool       pyramid

â”œâ”€ Brief          â”œâ”€ Tavily Search  â”œâ”€ MECE Check   â”œâ”€ Rubric Eval   â”œâ”€ Export
â”œâ”€ Governing      â”œâ”€ Evidence       â”œâ”€ Citation     â”œâ”€ Revision Plan  â”œâ”€ Markdown
   Thoughts          Collection         Stitching   â”œâ”€ Quality Score  â”œâ”€ JSON
â”œâ”€ MECE Reasons   â”œâ”€ Confidence     â”œâ”€ Pyramid      â””â”€ Recommendations â””â”€ Artifacts
â”œâ”€ Evidence Tasks    Scoring           Format
â””â”€ Risk List      â””â”€ Provenance
```

---

## ğŸš€ Quick Start

### **1. Deploy (Same as Before)**

```bash
cd Desktop\minto-pyramid-mcp
git add .
git commit -m "v3.0: Reasoning Orchestrator"
git push origin main
```

FastMCP Cloud auto-deploys in 2-3 minutes.

### **2. Connect to Claude Desktop**

```json
{
  "mcpServers": {
    "minto-pyramid": {
      "url": "https://PyramidlogicMINTOmindrian.fastmcp.app/mcp"
    }
  }
}
```

### **3. Run Complete Analysis**

```
Use the full_pyramid_analysis prompt with:

Brief: "Our revenue grew 60% but profit margin dropped from 28% to 14%. 
Customer acquisition cost tripled. Analyze this paradox."

Audience: board_of_directors
```

**Claude will automatically orchestrate all 5 stages!**

---

## ğŸ› ï¸ The 5 Orchestrator Tools

### **1. `plan_pyramid`** - Strategic Planning

```python
plan_pyramid(
    brief="Your business challenge",
    audience="executives",
    constraints={"timeframe": "Q1 2025"}
)
```

**Returns:**
- `run_id` - Unique identifier for this analysis
- `governing_thoughts` - 3 hypothesis candidates
- `reasons` - MECE categories (4-6)
- `evidence_tasks` - Tavily search tasks per reason
- `mece_score` - Quality metric (0.0-1.0)
- `risks` - Identified assumptions and gaps

**Use when:** Starting a new analysis

---

### **2. `run_plan_stage`** - Evidence Gathering

```python
run_plan_stage(
    run_id="abc123",
    stage="all"  # or "reason_1", "reason_2", etc.
)
```

**Does:**
- Executes all Tavily searches in parallel
- Collects evidence with URLs and confidence scores
- Tracks provenance (query, rank, timestamp)
- Normalizes results for citation

**Returns:**
- Evidence collected per reason
- Average confidence scores
- Total evidence count

**Use when:** Collecting supporting evidence

---

### **3. `synthesize_pyramid`** - Document Generation

```python
synthesize_pyramid(
    run_id="abc123",
    include_critique=True
)
```

**Quality Gates:**
- âœ… MECE score â‰¥ 0.75
- âœ… Evidence count â‰¥ 2Ã— reasons
- âœ… No critical contradictions

**Returns:**
- Complete Markdown deliverable
- Executive summary (governing thought)
- MECE reasons with evidence
- Full citation list with hyperlinks
- Quality metrics

**Use when:** Ready to generate output

---

### **4. `critique_pyramid_tool`** - Quality Evaluation

```python
critique_pyramid_tool(
    run_id="abc123"
)
```

**Evaluates:**
1. **Pyramid Fidelity** - MECE compliance, structure
2. **Evidence Sufficiency** - Coverage and confidence
3. **Consistency** - Contradiction detection

**Returns:**
- Per-aspect scores (0.0-1.0)
- Findings and recommendations
- Revision plan if needed
- Overall quality gate (pass/fail)

**Use when:** Validating before finalization

---

### **5. `finalize_pyramid`** - Export

```python
finalize_pyramid(
    run_id="abc123",
    export_format="both"  # markdown, json, or both
)
```

**Exports:**
- **Markdown:** Full formatted report with citations
- **JSON:** Structured data for programmatic use
- **Metrics:** Quality scores and statistics

**Use when:** Analysis complete and validated

---

## ğŸ“Š Complete Workflow Example

```
User: "Analyze declining profit margins in our SaaS business"

Claude orchestrates:

1ï¸âƒ£ plan_pyramid(brief="declining profit margins SaaS")
   â†’ run_id: "a1b2c3"
   â†’ 4 MECE reasons
   â†’ 8 evidence tasks
   â†’ MECE score: 0.85 âœ“

2ï¸âƒ£ run_plan_stage(run_id="a1b2c3", stage="all")
   â†’ Tavily searches: 8 queries
   â†’ Evidence collected: 24 items
   â†’ Avg confidence: 0.82

3ï¸âƒ£ synthesize_pyramid(run_id="a1b2c3")
   â†’ Quality gates: PASS âœ“
   â†’ Citations: 24 sources
   â†’ Output: 2,500 word report

4ï¸âƒ£ critique_pyramid_tool(run_id="a1b2c3")
   â†’ Pyramid fidelity: 0.85
   â†’ Evidence sufficiency: 0.88
   â†’ Consistency: 0.92
   â†’ Overall: 0.88 (PASS) âœ“

5ï¸âƒ£ finalize_pyramid(run_id="a1b2c3", format="both")
   â†’ Markdown âœ“
   â†’ JSON âœ“
   â†’ Artifacts exported âœ“

Output includes:
âœ… Executive summary
âœ… 4 MECE reasons with evidence
âœ… 24 cited sources with hyperlinks
âœ… Opportunities identified
âœ… Strategic recommendations
âœ… Quality metrics
```

---

## ğŸ“š Resources (Access Run Artifacts)

```
pyramid://runs/{run_id}/plan         # ReasoningPlan JSON
pyramid://runs/{run_id}/evidence     # All evidence items
pyramid://runs/{run_id}/deliverable  # Final markdown
pyramid://runs/{run_id}/metrics      # Quality scores
```

**Example:**
```
Show me pyramid://runs/a1b2c3/metrics
```

---

## ğŸ¯ Key Concepts

### **ReasoningPlan**
Structured execution plan with:
- Governing thought hypotheses
- MECE reasons
- Evidence tasks (dependencies, acceptance criteria)
- Risk list
- Execution sequence

### **MECE Validation**
Automated check for:
- **Mutually Exclusive:** No overlaps between reasons
- **Collectively Exhaustive:** No gaps in coverage
- **Same Level:** Consistent abstraction

Scored 0.0-1.0, must be â‰¥0.75 to synthesize.

### **Evidence Provenance**
Every evidence item tracks:
- Source query
- Search engine
- Rank position
- Timestamp
- Confidence score
- URL for citation

### **Critique Loop**
Systematic evaluation against rubric:
1. Pyramid fidelity (MECE, top-down)
2. Evidence sufficiency (coverage, confidence)
3. Consistency (contradictions)

Returns revision plan if quality gates fail.

---

## ğŸ’¡ Why This Architecture?

### **Problem:** LLMs are non-deterministic and stateless
- Forget context between calls
- Inconsistent reasoning paths
- No validation or quality control
- Poor evidence tracking

### **Solution:** Explicit reasoning scaffold
- âœ… Deterministic stages with checkpoints
- âœ… State persistence across calls
- âœ… Automated quality gates
- âœ… Full provenance and citations
- âœ… Measurable outputs

### **Benefit:** Reproducible, auditable analysis
- Run same brief twice â†’ same structure
- Trace every decision
- Validate quality objectively
- Iterate with targeted revisions

---

## ğŸ”¬ Quality Metrics

### **MECE Score**
```
Score = 1.0 - (overlap_penalty + gap_penalty)
Pass threshold: â‰¥ 0.75
```

### **Evidence Sufficiency**
```
Score = (evidence_count / (reasons Ã— 2)) Ã— avg_confidence
Pass threshold: â‰¥ 0.70
```

### **Contradiction Score**
```
Score = contradiction_count Ã— severity_weight
Pass threshold: â‰¤ 0.30
```

### **Overall Quality**
```
Overall = (pyramid_fidelity + evidence + consistency) / 3
Pass threshold: â‰¥ 0.75
```

---

## ğŸ§ª Testing

### **Simple Test:**
```
Use plan_pyramid to analyze: "Our customer churn increased from 5% to 18%"
```

### **Full Pipeline:**
```
Use the full_pyramid_analysis prompt with:
Brief: "Revenue up 60%, profits down to 14%"
Audience: executives
```

### **Check Artifacts:**
```
After running, show me:
- pyramid://runs/{run_id}/metrics
- pyramid://runs/{run_id}/deliverable
```

---

## ğŸ†š Comparison: v2.0 vs v3.0

| Feature | v2.0 | v3.0 |
|---------|------|------|
| **Architecture** | Individual tools | 5-stage pipeline |
| **Planning** | Manual | Automated ReasoningPlan |
| **MECE** | Manual check | Automated scoring |
| **Evidence** | Ad-hoc search | Orchestrated with provenance |
| **Quality** | Subjective | Measured with gates |
| **State** | Stateless | Persistent RunState |
| **Iteration** | Manual | Critique loop |
| **Observability** | Logs only | Metrics + artifacts |
| **Reproducibility** | Low | High |

---

## ğŸ”® What's Next (Future Enhancements)

### **Phase 2:**
- [ ] Multi-agent mode (Planner, Researcher, Critic, Writer)
- [ ] Counter-argument generation per reason
- [ ] Source diversity constraints
- [ ] Automatic ablation testing

### **Phase 3:**
- [ ] Neo4j GraphRAG integration
- [ ] Embeddings for semantic MECE validation
- [ ] Real NLI model for contradictions
- [ ] OpenTelemetry tracing

### **Phase 4:**
- [ ] LangFlow node wrapper
- [ ] PDF export with diagrams
- [ ] Multi-run comparison
- [ ] Parallel hypothesis testing

---

## ğŸ“ Support

**GitHub:** https://github.com/jsagir/Mindrian_Minto-MCP  
**Server:** https://PyramidlogicMINTOmindrian.fastmcp.app/mcp  
**Version:** 3.0.0  
**Status:** ğŸŸ¢ Live

---

## ğŸ“ For Developers

### **Extending the Orchestrator:**

1. **Add new quality gates:**
   ```python
   # In reasoning/metrics.py
   def custom_quality_check(plan: ReasoningPlan) -> float:
       # Your logic
       return score
   ```

2. **Add new evidence sources:**
   ```python
   # In reasoning/execution.py
   async def execute_custom_task(task: EvidenceTask):
       # Your API integration
       return evidence_items
   ```

3. **Customize synthesis:**
   ```python
   # In reasoning/pyramid.py
   def custom_deliverable_format(plan, evidence):
       # Your template
       return formatted_output
   ```

---

## ğŸ† Built For

This is the **prototype for Lawrence's distributed intelligence vision:**

âœ… **Workflows as Intelligence** - Each MCP = specialized reasoning  
âœ… **Composable** - Chain MCPs together  
âœ… **Auditable** - Full provenance and metrics  
âœ… **Reproducible** - Same input â†’ same structure  
âœ… **Scalable** - Run 1000s of analyses

**If this works, we can build MCPs for:**
- Market research workflows
- Competitive analysis  
- OKR frameworks
- Growth strategies
- Customer research
- Product strategy
- Financial analysis
- Risk assessment

**Each becomes a specialized "intelligence" that Claude orchestrates.**

---

**ğŸš€ Deploy now and start building the intelligence layer!**

```bash
git add .
git commit -m "v3.0: Full Reasoning Orchestrator"
git push
```

Your enhanced server will be live in 3 minutes! ğŸ‰
